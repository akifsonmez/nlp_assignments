{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ducc6iq3VmZS",
        "colab_type": "text"
      },
      "source": [
        "# 1) Installing Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu10osVSUzvs",
        "colab_type": "code",
        "outputId": "6b26ea5e-8d07-49da-aa42-db3658e82bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "!pip install dynet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dynet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/f0/01a561a301a8ea9aea1c28f82e108c38cd103964c7a46286ab01757a4092/dyNET-2.1-cp36-cp36m-manylinux1_x86_64.whl (28.1MB)\n",
            "\u001b[K     |████████████████████████████████| 28.1MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dynet) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from dynet) (0.29.19)\n",
            "Installing collected packages: dynet\n",
            "Successfully installed dynet-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSQ_4sQaVwjb",
        "colab_type": "text"
      },
      "source": [
        "# 2) Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vwAKxUYUmv0",
        "colab_type": "code",
        "outputId": "90b16f28-0cf9-4a81-a3ff-9df2184552ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import dynet as dy\n",
        "import nltk\n",
        "import math\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESmZQPezV0sa",
        "colab_type": "text"
      },
      "source": [
        "# 3) Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCwjANcUqK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_glove():\n",
        "    word2vec = {}\n",
        "    embeddings = []\n",
        "    word2idx = {}\n",
        "    with open('/content/drive/My Drive/SPRING 2020/BBM497/Assignment4/glove.6B.50d.txt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            word2idx[word] = len(embeddings)\n",
        "            vec = np.asarray(values[1:], dtype='float32')\n",
        "            word2vec[word] = vec\n",
        "            embeddings.append(vec)\n",
        "    return np.array(embeddings), word2idx\n",
        "\n",
        "\n",
        "def read_data():\n",
        "    poems = []\n",
        "    with open(\"/content/drive/My Drive/SPRING 2020/BBM497/Assignment4/unim_poem.json\") as data:\n",
        "        poems_and_ids = json.loads(data.read())\n",
        "    for p in poems_and_ids[:100]:\n",
        "        poems.append(p[\"poem\"])\n",
        "    return poems\n",
        "\n",
        "\n",
        "def add_tags(poems):\n",
        "    for i, p in enumerate(poems):\n",
        "        p = p.replace(\"\\n\", \" eol \")\n",
        "        poems[i] = \"bos {} eos\".format(p)\n",
        "    return poems\n",
        "\n",
        "\n",
        "def create_bi_grams(poems_with_tags):\n",
        "    poems_bi_gram = []\n",
        "    for p in poems_with_tags:\n",
        "        p_bi_gram = list(nltk.bigrams(p.split()))\n",
        "        poems_bi_gram.extend(p_bi_gram)\n",
        "    return poems_bi_gram\n",
        "\n",
        "\n",
        "def get_vocab(poems_with_tags):\n",
        "    vocab = []\n",
        "    for p in poems_with_tags:\n",
        "        vocab.extend(p.split())\n",
        "    vocab = list(set(vocab))\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def get_probs(word, params, word2idx):\n",
        "    dy.renew_cg()\n",
        "\n",
        "    W1 = params[\"pW1\"]\n",
        "    b1 = params[\"pb1\"]\n",
        "    W2 = params[\"pW2\"]\n",
        "    b2 = params[\"pb2\"]\n",
        "    lookup_table = params[\"look_up\"]\n",
        "\n",
        "    try:\n",
        "        x = lookup_table[word2idx[word]]\n",
        "    except KeyError:\n",
        "        x = lookup_table[word2idx[\"unk\"]]\n",
        "\n",
        "    U = dy.tanh(W1 * x + b1)\n",
        "    y = W2 * U + b2\n",
        "    probs = dy.softmax(y)\n",
        "    return probs\n",
        "\n",
        "\n",
        "def predict_next_word(word, vocab, params, word2idx):\n",
        "    probs = get_probs(word, params, word2idx)\n",
        "    probs = probs.npvalue()\n",
        "\n",
        "    cum_prob = np.cumsum(probs)\n",
        "    random_prob = np.random.rand()\n",
        "\n",
        "    predicted_word_prob = min(cum_prob[cum_prob >= random_prob])\n",
        "    predicted_word_index = list(cum_prob).index(predicted_word_prob)\n",
        "    predicted_word = vocab[predicted_word_index]\n",
        "    return predicted_word\n",
        "\n",
        "\n",
        "def generate_poem(vocab, params, word2idx):\n",
        "    initial = \"bos\"\n",
        "    num_of_new_line = 0\n",
        "    poem = [\"bos\"]\n",
        "    while True:\n",
        "        next_word = predict_next_word(initial, vocab, params, word2idx)\n",
        "        initial = next_word\n",
        "\n",
        "        if next_word == \"eol\":\n",
        "            num_of_new_line += 1\n",
        "        if next_word == \"eos\" or num_of_new_line == 5:\n",
        "            poem.append(next_word)\n",
        "            break\n",
        "        poem.append(next_word)\n",
        "    return \" \".join(poem)\n",
        "\n",
        "\n",
        "def calculate_perplexity(poem, params, word2idx, vocab_word_to_index):\n",
        "    poem_words = poem.split()\n",
        "    poem_bi_gram = list(nltk.bigrams(poem_words))\n",
        "\n",
        "    total_probs = 0\n",
        "    for w1, w2 in poem_bi_gram:\n",
        "        probs = get_probs(w1, params, word2idx)\n",
        "        probs = probs.npvalue()\n",
        "        total_probs += math.log2(probs[vocab_word_to_index[w2]])\n",
        "    perplexity = 2 ** ((-1 / len(poem_words)) * total_probs)\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McKHN9DC1yXX",
        "colab_type": "text"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3a5IxzG2kYT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "511dc21c-0cac-439f-9834-62bbf51ddb5d"
      },
      "source": [
        "embeddings, word2idx = read_glove()  # read glove\n",
        "poems = read_data()  # read poems\n",
        "poems_with_tags = add_tags(poems)  # add tags to poems\n",
        "vocab = get_vocab(poems_with_tags)  # get vocab of the poems\n",
        "poems_bi_gram = create_bi_grams(poems_with_tags)  # create bi_gram of the poems\n",
        "\n",
        "vocab_word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "hidden_neuron_size = 50\n",
        "input_neuron_size = 50\n",
        "EPOCHS = 20\n",
        "\n",
        "model = dy.Model()\n",
        "trainer = dy.SimpleSGDTrainer(model)\n",
        "\n",
        "pW1 = model.add_parameters((hidden_neuron_size, input_neuron_size))\n",
        "pb1 = model.add_parameters(hidden_neuron_size)\n",
        "pW2 = model.add_parameters((vocab_size, hidden_neuron_size))\n",
        "pb2 = model.add_parameters(vocab_size)\n",
        "lookup_table = model.add_lookup_parameters((len(word2idx), input_neuron_size), init=embeddings)\n",
        "\n",
        "params = {\"pW1\": pW1, \"pW2\": pW2, \"pb1\": pb1, \"pb2\": pb2, \"look_up\": lookup_table}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  epoch_loss = 0.0\n",
        "  for (w1, w2) in poems_bi_gram:\n",
        "    probs = get_probs(w1, params, word2idx)\n",
        "    loss = -dy.log(dy.pick(probs, vocab_word_to_index[w2]))\n",
        "    epoch_loss += loss.scalar_value()\n",
        "\n",
        "    loss.backward()\n",
        "    trainer.update()\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(\"Epoch %d. loss = %f\" % (epoch, epoch_loss))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. loss = 26508.540470\n",
            "Epoch 1. loss = 22741.818008\n",
            "Epoch 2. loss = 20066.946369\n",
            "Epoch 3. loss = 18006.316627\n",
            "Epoch 4. loss = 16313.344533\n",
            "Epoch 5. loss = 15071.908056\n",
            "Epoch 6. loss = 14218.855844\n",
            "Epoch 7. loss = 13629.933046\n",
            "Epoch 8. loss = 13150.505507\n",
            "Epoch 9. loss = 12892.602535\n",
            "Epoch 10. loss = 12709.631286\n",
            "Epoch 11. loss = 12665.339040\n",
            "Epoch 12. loss = 12534.489432\n",
            "Epoch 13. loss = 12537.475384\n",
            "Epoch 14. loss = 12356.554114\n",
            "Epoch 15. loss = 12302.613100\n",
            "Epoch 16. loss = 12410.077007\n",
            "Epoch 17. loss = 12404.600083\n",
            "Epoch 18. loss = 12515.664844\n",
            "Epoch 19. loss = 12525.350399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKxs3zc7zvT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "ac97a853-a895-4e01-db60-21c0e6cec08e"
      },
      "source": [
        "generated_poem = generate_poem(vocab, params, word2idx)\n",
        "print(generated_poem.replace(\"eol\", \"\\n\"))\n",
        "print(\"Perplexity of the generated sentence: %.3f\" % calculate_perplexity(generated_poem, params, word2idx, vocab_word_to_index))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bos he heard my heart \n",
            " only thing \n",
            " wondered if he won my heart \n",
            " i thought my heart \n",
            " only thing was \n",
            "\n",
            "Perplexity of the generated sentence: 3.198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8o8CG9G2OXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}